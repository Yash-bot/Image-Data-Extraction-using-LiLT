# -*- coding: utf-8 -*-
"""Cheque data extraction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apjZ9VGcwtYSPRVcAQrH7tR7YcxQ9HAd

# Import
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install azure-ai-formrecognizer
# %pip install pdf2image
# %pip install amazon-textract-response-parser
# %pip install stats
# %pip install matplotlib
# %pip install ipywidgets
# %pip install transformers[torch]
# %pip install transformers datasets seqeval tensorboard --upgrade
# %pip install evaluate
# %pip install ipywidgets

# install git-fls for pushing model and logs to the hugging face hub
!apt-get install git-lfs --yes
!apt update -y
!apt install poppler-utils -y

import sys
import os
import json
import random
import numpy as np
import pickle
import torch
from datasets import Dataset
import numpy as np
from transformers import Trainer, TrainingArguments, LayoutLMv3Processor, LiltForTokenClassification, LayoutLMv3FeatureExtractor, AutoTokenizer, LayoutLMv3Processor
from datasets import Dataset, DatasetDict, Features, Sequence, ClassLabel, Value, Array2D
import matplotlib.pyplot as plt
from glob import glob
from typing import Optional
from PIL import Image, ImageOps, ImageFont, ImageDraw
from tqdm.notebook import tqdm  # Progress bars
from prepare_model_inputs import read_text_from_image
from config import labels, id2label, labels, labels_wto_other, label2color, label2id

from google.colab import drive
drive.mount('/content/drive')

"""# Data"""

!cp -r /content/drive/MyDrive/azure

!unzip /content/drive/MyDrive/azure/Images.zip

"""# OCR"""

ocr_dir = 'data/ocr_files'
os.makedirs(ocr_dir, exist_ok=True)

def return_path_data(file_path):
    dir_name, file_name = os.path.split(file_path)
    file_name_wto_ext, ext = os.path.splitext(file_name)
    return dir_name, file_name, file_name_wto_ext, ext

files = glob('/content/Images/*')
files[:3]

len(files)

import os
from io import BytesIO
from azure.core.credentials import AzureKeyCredential
from azure.ai.formrecognizer import DocumentAnalysisClient
import json
import math
from PIL import Image
KEY = '713e2c25fec145d7928e0f697483fcc0'
ENDPOINT = 'https://yash04.cognitiveservices.azure.com/'
MODEL_ID = 'prebuilt-read'

def read_text_from_image(pil_image):
    # Initialize the Azure Form Recognizer client
    document_analysis_client = DocumentAnalysisClient(endpoint=ENDPOINT, credential=AzureKeyCredential(KEY))

    # Convert the PIL image to bytes
    image_bytes = BytesIO()
    pil_image.save(image_bytes, format="PNG")
    image_bytes.seek(0)
    # Analyze the image asynchronously
    try:
        poller = document_analysis_client.begin_analyze_document(MODEL_ID, document=image_bytes)
        ocr_output = poller.result()
        ocr_output = ocr_output.to_dict()
        return ocr_output
    except Exception as e:
        print(e)

clean_img_dir = 'data/clean_images'
os.makedirs(clean_img_dir, exist_ok=True)

data = {}
failed_items = []
for image_path in tqdm(files):
    image = Image.open(image_path)
    image = ImageOps.exif_transpose(image)
    image = image.convert('RGB')
    file_name = os.path.basename(image_path)
    new_image_path = f"{clean_img_dir}/{file_name}"
    image.save(new_image_path)
    filename_wto_ext= return_path_data(image_path)[2]
    ocr_filename = f"{filename_wto_ext}.json"
    ocr_file_path = os.path.join(ocr_dir, ocr_filename)
    if not os.path.exists(ocr_file_path):
        print('calling ocr api')
        try:
            ocr_data = read_text_from_image(image)
            if ocr_data is not None:
                with open(ocr_file_path, 'w', encoding='utf-8') as f:
                    json.dump(ocr_data, f, ensure_ascii=False)
            else:
                ocr_file_path = None
                print('ocr output is none')
                print(file_name)
        except Exception as e:
            print(e)
            failed_items.append(file_name)
            ocr_file_path = None
            print('ocr call failed')
            print(file_name)

    data[file_name] = {'image': image, 'image_path': new_image_path, 'ocr_file_path': ocr_file_path}

ocr_files = glob('data/ocr_files/*')

len(ocr_files)



failed_items

data['Cheque 309137.png']



"""# Load Annotations"""

with open('/content/project-2-at-2024-06-12-11-20-92d0bdaf.json') as f:
    labelled_data = json.load(f)

len(labelled_data)

random.seed(42)
random.shuffle(labelled_data)

def split_dataset(data, val_size=.20, test_size=.10):
    train_data = []
    test_data = []
    val_data = []
    val_size = int(len(data)*val_size)

    test_size = int(len(data)*test_size)

    train_size = len(data) - val_size - test_size

    for i, sample in enumerate(data):
        if i < train_size:
            train_data.append(sample)
        elif i < (train_size + val_size):
            val_data.append(sample)
        else:
            test_data.append(sample)

    return train_data, val_data, test_data

train_data, val_data, test_data = split_dataset(labelled_data)

train_data_path = 'data/train_data.json'
with open(train_data_path, 'w') as f:
    json.dump(train_data, f)

val_data_path = 'data/val_data.json'
with open(val_data_path, 'w') as f:
    json.dump(val_data, f)

test_data_path = 'data/test_data.json'
with open(test_data_path, 'w') as f:
    json.dump(test_data, f)

with open(train_data_path) as f:
    train_data = json.load(f)

with open(val_data_path) as f:
    val_data = json.load(f)

with open(test_data_path) as f:
    test_data = json.load(f)

len(train_data), len(val_data), len(test_data)

"""# Map labels to words

## Convert textract and SMGT labels into the Hugging face dataset
"""

class AnnotationBoundingBox:
    """Class to parse a bounding box annotated by SageMaker Ground Truth Object Detection

    Pre-calculates all box TLHWBR metrics (both absolute and relative) on init, for efficient and
    easy processing later.
    """

    def __init__(self, manifest_box: dict, image_height: int, image_width: int):
        self._class_id = manifest_box["class_id"]
        self._abs_top = manifest_box["top"]
        self._abs_left = manifest_box["left"]
        self._abs_height = manifest_box["height"]
        self._abs_width = manifest_box["width"]
        self._abs_bottom = self.abs_top + self.abs_height
        self._abs_right = self.abs_left + self.abs_width
        self._rel_top = self._abs_top / image_height
        self._rel_left = self._abs_left / image_width
        self._rel_height = self._abs_height / image_height
        self._rel_width = self._abs_width / image_width
        self._rel_bottom = self._abs_bottom / image_height
        self._rel_right = self._abs_right / image_width

    @property
    def class_id(self):
        return self._class_id

    @property
    def abs_top(self):
        return self._abs_top

    @property
    def abs_left(self):
        return self._abs_left

    @property
    def abs_height(self):
        return self._abs_height

    @property
    def abs_width(self):
        return self._abs_width

    @property
    def abs_bottom(self):
        return self._abs_bottom

    @property
    def abs_right(self):
        return self._abs_right

    @property
    def rel_top(self):
        return self._rel_top

    @property
    def rel_left(self):
        return self._rel_left

    @property
    def rel_height(self):
        return self._rel_height

    @property
    def rel_width(self):
        return self._rel_width

    @property
    def rel_bottom(self):
        return self._rel_bottom

    @property
    def rel_right(self):
        return self._rel_right

class BoundingBoxAnnotationResult():
    def __init__(self, manifest_obj: dict):
        """Initialize a BoundingBoxAnnotationResult

        Arguments
        ---------
        manifest_obj : dict
            The contents of the output field of a record in a SMGT Object Detection labelling job
            output manifest, or equivalent.
        """
        try:
            image_size_spec = manifest_obj["image_size"][0]
            self._image_height = int(image_size_spec["height"])
            self._image_width = int(image_size_spec["width"])
            self._image_depth = (
                int(image_size_spec["depth"]) if "depth" in image_size_spec else None
            )
        except Exception as e:
            raise ValueError(
                "".join(
                    (
                        "manifest_obj must be a dictionary including 'image_size': a list of ",
                        "length 1 whose first/only element is a dict with integer properties ",
                        f"'height' and 'width', optionally also 'depth'. Got: {manifest_obj}",
                    )
                )
            ) from e
        assert (
            len(manifest_obj["image_size"]) == 1
        ), f"manifest_obj['image_size'] must be a list of len 1. Got: {manifest_obj['image_size']}"

        try:
            self._boxes = [
                AnnotationBoundingBox(
                    b,
                    image_height=self._image_height,
                    image_width=self._image_width,
                )
                for b in manifest_obj["annotations"]
            ]
        except Exception as e:
            raise ValueError(
                "".join(
                    (
                        "manifest_obj['annotations'] must be a list-like of absolute TLHW bounding box ",
                        f"dicts with class_id. Got {manifest_obj['annotations']}",
                    )
                )
            ) from e

    @property
    def image_height(self):
        return self._image_height

    @property
    def image_width(self):
        return self._image_width

    @property
    def image_depth(self):
        return self._image_depth

    @property
    def boxes(self):
        return self._boxes

    def normalized_boxes(
        self,
        return_tensors: Optional[str] = None,
    ):
        """Annotation boxes in 0-1000 normalized x0,y0,x1,y1 array/tensor format as per LayoutLM"""
        raw_zero_to_one_list = [
            [
                box.rel_left,
                box.rel_top,
                box.rel_right,
                box.rel_bottom,
            ]
            for box in self._boxes
        ]
        if return_tensors == "np" or not return_tensors:
            if len(raw_zero_to_one_list) == 0:
                npresult = np.zeros((0, 4), dtype="long")
            else:
                npresult = (np.array(raw_zero_to_one_list) * 1000).astype("long")
            return npresult if return_tensors else npresult.tolist()

def word_label_matrix_from_norm_bounding_boxes(
    boxes: np.array,  # TODO: PyTorch Tensor support?
    smgt_boxes_ann: BoundingBoxAnnotationResult,
    n_classes: int,
) -> np.ndarray:
    """Calculate (multi-label) word classes by overlap of Textract (TRP) items with SMGT BBoxes

    Parameters
    ----------
    textract_blocks :
        List-like of TRP objects with a 'geometry' property (e.g. Word, Line, Cell, Page, etc)
    smgt_boxes_ann :
        Parsed result from a SageMaker Ground Truth Bounding Box annotation job
    n_classes :
        Number of classes in the annotation dataset

    Returns
    -------
    result : np.array
        (n_textract_blocks, n_classes) matrix of 0|1 class labels, defined as 1 where half or more
        of the block's area intersects with a bounding box annotation of that class. Note that
        multi-classification is supported so rows may sum to more than 1.
    """
    n_words = len(boxes)  # (n_words, 4) 0-1000 x0, y0, x1, y1
    ann_boxes = smgt_boxes_ann.normalized_boxes(return_tensors="np")
    if len(ann_boxes) == 0:
        # Easier to just catch this case than deal with it later:
        return np.concatenate(
            [np.zeros((n_words, n_classes - 1)), np.ones((n_words, 1))],
            axis=1,
        )
    ann_class_ids = np.array([box.class_id for box in smgt_boxes_ann.boxes])
    n_anns = len(ann_boxes)  # (n_words, 4) 0-1000 x0, y0, x1, y1

    word_widths = boxes[:, 2] - boxes[:, 0]
    word_heights = boxes[:, 3] - boxes[:, 1]
    word_areas = word_widths * word_heights

    # We want to produce a matrix (n_words, n_anns) describing overlaps
    # Note the slicing e.g. boxes[:, 2:3] vs boxes[:, 2] keeps the result a vector instead of 1D
    # array (as 1D array would mess up the tiling)
    isects_right = np.minimum(
        np.tile(boxes[:, 2:3], (1, n_anns)),
        np.tile(ann_boxes[:, 2:3].transpose(), (n_words, 1)),
    )
    isects_left = np.maximum(
        np.tile(boxes[:, 0:1], (1, n_anns)),
        np.tile(ann_boxes[:, 0:1].transpose(), (n_words, 1)),
    )
    isects_width = np.maximum(0, isects_right - isects_left)
    isects_bottom = np.minimum(
        np.tile(boxes[:, 3:4], (1, n_anns)),
        np.tile(ann_boxes[:, 3:4].transpose(), (n_words, 1)),
    )
    isects_top = np.maximum(
        np.tile(boxes[:, 1:2], (1, n_anns)),
        np.tile(ann_boxes[:, 1:2].transpose(), (n_words, 1)),
    )
    isects_height = np.maximum(0, isects_bottom - isects_top)

    matches = np.where(
        # (Need to convert word_areas from 1D array to column vector)
        isects_width * isects_height >= (word_areas / 2)[:, np.newaxis],
        1.0,
        0.0,
    )

    # But `matches` is not the final result: We want a matrix by class IDs, not every bounding box
    result = np.zeros((n_words, n_classes))
    # Not aware of any way to do this without looping yet:
    for class_id in range(n_classes):
        class_matches = np.any(matches[:, ann_class_ids == class_id], axis=1)
        result[:, class_id] = class_matches
    # Implicitly any word with no matches is "other", class n-1:
    result[:, n_classes - 1] = np.where(
        np.any(result, axis=1),
        result[:, n_classes - 1],
        1.0,
    )

    return result


def word_single_labels_from_norm_bounding_boxes(
    boxes: np.array,
    smgt_boxes_ann: BoundingBoxAnnotationResult,
    n_classes: int,
):
    """Assign a class label to each Textract (TRP) block based on SMGT Bounding Box overlaps

    Any words matching multiple classes' annotations are silently tagged to the lowest matched
    class index. Words with no annotation are tagged to the 'other' class (n_classes - 1).

    Parameters
    ----------
    boxes :
        Array of normalized LayoutLM-like word bounding boxes
    smgt_boxes_ann :
        Parsed result from a SageMaker Ground Truth Bounding Box annotation job
    n_classes :
        Number of classes in the annotation dataset INCLUDING the 'other' class (implicitly =
        n_classes - 1)

    Returns
    -------
    result : np.array
        (n_textract_blocks,) array of class label integers by word, from 0 to n_classes - 1
        inclusive.
    """
    word_labels = word_label_matrix_from_norm_bounding_boxes(boxes, smgt_boxes_ann, n_classes)
    return np.where(
        np.sum(word_labels, axis=1) == 0,
        n_classes - 1,
        np.argmax(word_labels, axis=1),
    )

def load_textract_data(ocr_path):
    extracted_data = []
    with open(ocr_path, 'r', encoding='utf-8') as f:
        ocr_data = json.load(f)
    page_data = ocr_data['pages'][0]
    words = page_data['words']
    width = page_data['width']
    height = page_data['height']
    for word in words:
        word_content = word['content']
        polygons = word['polygon']
        x_values = [point['x'] for point in polygons]
        y_values = [point['y'] for point in polygons]

        left = min(x_values)/width
        right = max(x_values)/width
        top = min(y_values)/height
        bottom = max(y_values)/height
        bbox = {
                "Width": right- left,
                "Height": bottom-top,
                "Left": left,
                "Top": top
            }

        extracted_data.append({"text": word_content, "bbox": bbox})

    return extracted_data

def convert_textract_to_layoutlm_format(textract_data):
    """
    Convert Textract bounding boxes to a format compatible with the LayoutLM model, normalized between 0 and 1000.
    """
    boxes = []
    for word in textract_data:
        bbox = word['bbox']
        left, top, width, height = bbox['Left'], bbox['Top'], bbox['Width'], bbox['Height']
        right = left + width
        bottom = top + height
        boxes.append([left*1000, top*1000, right*1000, bottom*1000])
    return np.array(boxes)


def return_word_bounding_box(textract_data, image_width, image_height):
    """
    Convert Textract bounding boxes to a format compatible with the LayoutLM model, normalized between 0 and 1000.
    """
    boxes = []
    for word in textract_data:
        bbox = word['bbox']
        left, top, width, height = bbox['Left'], bbox['Top'], bbox['Width'], bbox['Height']
        right = left + width
        bottom = top + height
        boxes.append([left*image_width, top*image_height, right*image_width, bottom*image_height])

    return boxes


def return_word_labels(textract_data, smgt_data, n_classes):
    """
    Map SMGT bounding box labels to Textract extracted words.
    """
    # Convert Textract data to LayoutLM format

    boxes = convert_textract_to_layoutlm_format(textract_data)

    # Initialize BoundingBoxAnnotationResult with SMGT data
    smgt_boxes_ann = BoundingBoxAnnotationResult(smgt_data)

    # Calculate word labels based on bounding box overlap
    word_labels = word_single_labels_from_norm_bounding_boxes(boxes, smgt_boxes_ann, n_classes)

    return word_labels

n_classes = len(labels)
n_classes

def prepare_dataset(labelled_data):
    dataset = {"tokens": [], "bboxes": [], "ner_tags": [], "image": [], 'file_name': []}
    for line in labelled_data:
        image_size = {}
        results = line['annotations'][0]['result']
        annotations = []
        for result in results:
            if 'height' not in image_size:
                image_size['width'] = result['original_width']
                image_size['height'] = result['original_height']
                image_size['depth'] = 3

            val = result['value']

            annotation = {
                        "class_id": label2id[val['rectanglelabels'][0]],
                        "top": int((val['y']/100)*image_size['height']),
                        "left": int((val['x']/100)*image_size['width']),
                        "height": int((val['height']/100)*image_size['height']),
                        "width": int((val['width']/100)*image_size['width'])
                    }

            annotations.append(annotation)


        smgt_data = {
                'image_size':  [image_size],
                'annotations' : annotations
            }


        file_name = line['file_upload'].split('-')[-1].replace('_',' ')

        data_item = data[file_name]

        ocr_file_path = data_item['ocr_file_path']
        print(file_name)
        print(ocr_file_path)

        textract_data = load_textract_data(ocr_file_path)
        boxes_layoutlm_format = convert_textract_to_layoutlm_format(textract_data)
        smgt_boxes_ann = BoundingBoxAnnotationResult(smgt_data)
        labels = word_single_labels_from_norm_bounding_boxes(boxes_layoutlm_format, smgt_boxes_ann, n_classes).tolist()
        # labels = [id2label[x] for x in labels]
        dataset["tokens"].append([x['text'] for x in textract_data])
        dataset["ner_tags"].append(labels)

        image_pil = data_item['image']
        dataset['image'].append(image_pil)

        dataset['file_name'].append(file_name)

        image_width, image_height = image_pil.size
        words_bbox = return_word_bounding_box(textract_data, image_width, image_height)

        dataset["bboxes"].append(boxes_layoutlm_format)
        # dataset["filename"].append(filename)

    return Dataset.from_dict(dataset)

dataset_train = prepare_dataset(train_data)
dataset_train

dataset_val = prepare_dataset(val_data)
dataset_val

dataset_test = prepare_dataset(test_data)
dataset_test

dataset = DatasetDict({
    'train': dataset_train,
    'val': dataset_val,
    'test':dataset_test
})

print(dataset)

dataset['train'][0]['file_name']

def unnormalize_box(bbox, width, height):
    return [
        width * (bbox[0] / 1000),
        height * (bbox[1] / 1000),
        width * (bbox[2] / 1000),
        height * (bbox[3] / 1000),
    ]

def draw_boxes(image, boxes, labels, label2color=label2color):
    image_copy = image.copy()
    draw = ImageDraw.Draw(image_copy, "RGBA")
    font = ImageFont.load_default()
    width, height = image.size
    for label, box in zip(labels, boxes):
        box = unnormalize_box(box, width, height)
        box = [int(x) for x in box]
        label = id2label[label]
        color = label2color[label] + (128,)  # Adding transparency
        draw.rectangle(box, outline="black")
        draw.rectangle(box, fill=color, outline=color)
        if label != 'other':
            draw.text((box[0] + 10, box[1] - 10), text=f"{label}", fill=(0, 0, 0, 255), font=font)
    return image_copy

drawn_images_train = f'data/drawn_images_train'
os.makedirs(drawn_images_train, exist_ok = True)

dataset

for i in range(len(dataset['train'])):
    sample = dataset['train'][i]
    image = sample['image']
    boxes = sample['bboxes']
    labels = sample['ner_tags']
    drawn_img = draw_boxes(image, boxes,labels, label2color=label2color)
    filename = sample['file_name']
    drawn_img.save(f'{drawn_images_train}/{filename}')

drawn_images_val = f'data/drawn_images_val'
os.makedirs(drawn_images_val, exist_ok = True)

for i in range(len(dataset['val'])):
    sample = dataset['val'][i]
    image = sample['image']
    boxes = sample['bboxes']
    labels = sample['ner_tags']
    drawn_img = draw_boxes(image, boxes,labels, label2color=label2color)
    filename = sample['file_name']
    drawn_img.save(f'{drawn_images_val}/{filename}')

drawn_images_val = f'data/drawn_images_test'
os.makedirs(drawn_images_val, exist_ok = True)

for i in range(len(dataset['test'])):
    sample = dataset['test'][i]
    image = sample['image']
    boxes = sample['bboxes']
    labels = sample['ner_tags']
    drawn_img = draw_boxes(image, boxes,labels, label2color=label2color)
    filename = sample['file_name']
    drawn_img.save(f'{drawn_images_val}/{filename}')

# prompt: download drawn_images_test folder

!zip -r drawn_images_test.zip data/drawn_images_test
from google.colab import files
files.download("drawn_images_test.zip")

"""# Train"""

from huggingface_hub import notebook_login, HfFolder

notebook_login()

model_id="SCUT-DLVCLab/lilt-roberta-en-base"

feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False) # set
tokenizer = AutoTokenizer.from_pretrained(model_id)
processor = LayoutLMv3Processor(feature_extractor, tokenizer)

from functools import partial
# we need to define custom features
features = Features(
    {
        "input_ids": Sequence(feature=Value(dtype="int64")),
        "attention_mask": Sequence(feature=Value(dtype="int64")),
        "bbox": Array2D(dtype="int64", shape=(512, 4)),
        "labels": Sequence(ClassLabel(names=list(id2label.keys()))),
    }
)

# preprocess function to perpare into the correct format for the model
def process(sample, processor=None):
    encoding = processor(
        sample["image"].convert("RGB"),
        sample["tokens"],
        boxes=sample["bboxes"],
        word_labels=sample["ner_tags"],
        padding="max_length",
        truncation=True,
    )
    # remove pixel values not needed for LiLT
    del encoding["pixel_values"]
    return encoding


# process the dataset and format it to pytorch
proc_dataset = dataset.map(
    partial(process, processor=processor),
    remove_columns=["image", "tokens", "ner_tags", "bboxes", "file_name"],
    features=features,
).with_format("torch")

print(proc_dataset["train"].features.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox','lables'])

proc_dataset

proc_dataset['train'][0].keys()

model_id = "SCUT-DLVCLab/lilt-roberta-en-base"

# load model with correct number of labels and mapping
model = LiltForTokenClassification.from_pretrained(
    model_id, num_labels=len(label2id), label2id=label2id, id2label=id2label
)

"""We want to evaluate our model during training. The `Trainer` supports evaluation during training by providing a `compute_metrics`.
We are going to use `seqeval` and the `evaluate` library to evaluate the overall f1 score for all tokens.
"""

import evaluate
import numpy as np

# load seqeval metric
metric = evaluate.load("seqeval")

# labels of the model
ner_labels = list(model.config.id2label.values())


def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    all_predictions = []
    all_labels = []
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(ner_labels[predicted_idx])
            all_labels.append(ner_labels[label_idx])
    return metric.compute(predictions=[all_predictions], references=[all_labels])

# hugging face parameter
repository_id = "indudesane/Data_extraction"

# Define training args
training_args = TrainingArguments(
    output_dir=repository_id,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    fp16=True,
    learning_rate=5e-5,
    max_steps=2500,
    # logging & evaluation strategies
    logging_dir=f"{repository_id}/logs",
    logging_strategy="steps",
    logging_steps=200,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="overall_f1",
    # push to hub parameters
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_model_id=repository_id,
    hub_token=HfFolder.get_token(),
)

# Create Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=proc_dataset["train"],
    eval_dataset=proc_dataset["val"],
    compute_metrics=compute_metrics,
)

# Start training
trainer.train()

"""# Evaluate"""

trainer.evaluate()

"""# Save model data"""

# change apply_ocr to True to use the ocr text for inference
processor.feature_extractor.apply_ocr = True

# Save processor and create model card
processor.save_pretrained(repository_id)
trainer.create_model_card()
trainer.push_to_hub()

# prompt: delete drive folder

!rm -rf /content/drive

!zip -r dataset.zip /content/data

# prompt: downlaod dataset.zip

from google.colab import files
files.download("/content/dataset.zip")

